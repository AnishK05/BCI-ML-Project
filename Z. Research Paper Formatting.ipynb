{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["pp7XI3gn8C5g","PRh6PkIb27OB"],"authorship_tag":"ABX9TyOSRkSi8tLloDo89PdRaL3/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Download Datasets"],"metadata":{"id":"5fjj-IOOg3fc"}},{"cell_type":"code","source":["!pip install mne"],"metadata":{"id":"9AQ0dNoLL4uC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download BDF file\n","!wget -O eeg_data_02.bdf https://www.dropbox.com/scl/fi/et82hv39u24ll3brk8n0l/sub-02_ses-01_task-innerspeech_eeg.bdf?rlkey=prc1yt31nk1hb66h1qml4wkti&dl=0\n","!wget -O eeg_data_03.bdf https://www.dropbox.com/scl/fi/252j4sc1ymcibgitg57dv/sub-03_ses-01_task-innerspeech_eeg.bdf?rlkey=26o26pc7dgucbl5y60f9avd8y&dl=0\n","!wget -O eeg_data_04.bdf https://www.dropbox.com/scl/fi/m5nbdcgh2jsp4fqz1vl76/sub-04_ses-01_task-innerspeech_eeg.bdf?rlkey=07j2jydunn4u7nu36zee2nngl&dl=0\n","!wget -O eeg_data_05.bdf https://www.dropbox.com/scl/fi/0ob6affaz1nxyzmpif77r/sub-05_ses-01_task-innerspeech_eeg.bdf?rlkey=x6morhp7kho5xhc1bl5zm40fj&dl=0\n","!wget -O eeg_data_06.bdf https://www.dropbox.com/scl/fi/qgelieb2o3alzsh13kkq9/sub-06_ses-01_task-innerspeech_eeg.bdf?rlkey=suwr5st63vx43o6xzguyait0q&dl=0\n","!wget -O eeg_data_07.bdf https://www.dropbox.com/scl/fi/ln1tbvlcsdw4examr4uwo/sub-07_ses-01_task-innerspeech_eeg.bdf?rlkey=rktp9di84qv6ef8khk6iom84c&dl=0"],"metadata":{"id":"63NgaxkekzIq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Print EEG Data"],"metadata":{"id":"ifNJw1625TXw"}},{"cell_type":"code","source":["import mne\n","\n","file_paths = ['eeg_data_02.bdf', 'eeg_data_03.bdf', 'eeg_data_04.bdf',\n","              'eeg_data_05.bdf', 'eeg_data_06.bdf', 'eeg_data_07.bdf']\n","\n","for i in range(len(file_paths)):\n","  # Read the .bdf file\n","  raw = mne.io.read_raw_bdf(file_paths[i], preload=True)\n","  # Define a title for the plot\n","  plot_title = f\"EEG Data - {file_paths[i]}\"\n","  # Plot the data with the title\n","  raw.plot(title=plot_title)"],"metadata":{"id":"5er4e0Lv8xKl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# EEG Data Visualization\n","\n","\n","*   Raw EEG Data\n","*   Filtered EEG Data\n","\n"],"metadata":{"id":"i3DlBazxujH5"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Plotting raw EEG data\n","raw.plot(duration=5, n_channels=30)  # Adjust duration and n_channels as needed"],"metadata":{"id":"o2OImQSnuutT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming 'raw' is your filtered data\n","raw_filtered = raw.copy().filter(l_freq=8, h_freq=30, fir_design='firwin')\n","raw_filtered.plot(duration=5, n_channels=30)  # Adjust duration and n_channels as needed"],"metadata":{"id":"f6lY5tgZuvar"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Pre-Processing"],"metadata":{"id":"o9tbKWEug-GB"}},{"cell_type":"code","source":["import mne\n","\n","# Function to process each dataset\n","def process_dataset(file_path):\n","    # Read BDF File\n","    raw = mne.io.read_raw_bdf(file_path, preload=True)\n","    # Re-reference; NODE CHANGES HERE: 'EXG1', 'EXG2',\n","    # EXG3', 'EXG4', 'EXG5', 'EXG6', 'EXG7', 'EXG8'\n","    raw.set_eeg_reference(['EXG1', 'EXG2'])\n","    # Apply bandpass filter (8 to 30 Hz) and Notch\n","    # filter at 50 Hz\n","    raw.filter(l_freq=8, h_freq=30, fir_design='firwin')\n","    raw = mne.io.Raw.notch_filter(raw, freqs=50)\n","    # Decimate the data to reduce sampling rate to around 254 Hz\n","    raw.resample(254)\n","    # Epoching: Define events and epochs based on experimental design\n","    events = mne.find_events(raw, initial_event=True,\n","                             consecutive=True, min_duration=0.002)\n","    event_id = dict(Down=32, Left=33)\n","    picks_vir = mne.pick_types(raw.info, eeg=True, include=['EXG1',\n","                                                            'EXG2'], stim=False)\n","    epochs = mne.Epochs(raw, events, event_id=event_id, tmin=-0.5,\n","                        tmax=4, picks=picks_vir, preload=True, decim = 3, baseline=None)\n","    print(\"*****************************************************************************\")\n","    return epochs\n","\n","# Download and process each dataset\n","epochs_02 = process_dataset('eeg_data_02.bdf')\n","epochs_03 = process_dataset('eeg_data_03.bdf')\n","epochs_04 = process_dataset('eeg_data_04.bdf')\n","epochs_05 = process_dataset('eeg_data_05.bdf')\n","epochs_06 = process_dataset('eeg_data_06.bdf')\n","epochs_07 = process_dataset('eeg_data_07.bdf')\n","\n","# Combine epochs from all datasets\n","all_epochs = mne.concatenate_epochs([epochs_02, epochs_03, epochs_04,\n","                                     epochs_05, epochs_06, epochs_07])"],"metadata":{"id":"6Iq6HSLZSmFe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Epoch Visualization\n","\n","\n","*   Epochs Overview\n","*   Event-Related Potentials\n","\n"],"metadata":{"id":"ip1tjeoKu4nL"}},{"cell_type":"code","source":["# Assuming 'epochs' is your epochs data\n","all_epochs.plot(n_epochs=10)  # Adjust n_epochs to display a specific number of epochs"],"metadata":{"id":"gTlGIaFrvDnh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For each event type, plot the ERP\n","evoked_down = all_epochs['Down'].average()\n","evoked_down.plot(time_unit='s')\n","\n","evoked_left = all_epochs['Left'].average()\n","evoked_left.plot(time_unit='s')"],"metadata":{"id":"98SEBXN-vFdy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature Extraction\n","\n","import numpy as np\n","\n","# Compute PSD using the compute_psd method\n","spectrum = all_epochs.compute_psd(method='welch')\n","data, freqs = spectrum.get_data(return_freqs=True)\n","\n","# Skip conversion to dB, use the power values directly\n","psds_power = data\n","\n","# Mean PSD values across frequencies for each channel\n","features = psds_power.mean(axis=1)"],"metadata":{"id":"MOJPOYcUwqjT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PSD Plots"],"metadata":{"id":"d2CXymxCvbIM"}},{"cell_type":"code","source":["# For raw data\n","raw.plot_psd(fmin=2, fmax=40, average=True)\n","\n","# For filtered data\n","raw_filtered.plot_psd(fmin=2, fmax=40, average=True)"],"metadata":{"id":"n_CIffABveqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data Normalization\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(features)"],"metadata":{"id":"iHM2LmuLxD9A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dimensionality Reduction\n","\n","from sklearn.decomposition import PCA\n","\n","# Apply PCA for dimensionality reduction\n","pca = PCA(n_components=0.95)  # Keep 95% of variance\n","X_pca = pca.fit_transform(X_scaled)"],"metadata":{"id":"OgVqzH2Jy4UV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PCA Reduction\n","\n","*   Scree Plot\n","*   PCA Component Visualization\n","\n"],"metadata":{"id":"mk5mQgwdvjnR"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","\n","pca = PCA()\n","pca.fit(X_scaled)  # Assuming X_scaled is your standardized data\n","\n","plt.figure()\n","plt.plot(np.cumsum(pca.explained_variance_ratio_))\n","plt.xlabel('Number of Components')\n","plt.ylabel('Variance (%)')  # for each component\n","plt.title('Explained Variance')\n","plt.show()"],"metadata":{"id":"64D2ktKIvr_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure()\n","plt.scatter(X_pca[:, 0], X_pca[:, 1])  # Adjust indices for different components\n","plt.xlabel('PCA 1')\n","plt.ylabel('PCA 2')\n","plt.title('PCA of EEG Data')\n","plt.show()"],"metadata":{"id":"pq_ygwKzvuga"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Machine Learning Models"],"metadata":{"id":"EazwriO0hIEX"}},{"cell_type":"code","source":["# LDA Model Building\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.model_selection import train_test_split\n","\n","# Preparing labels for classification\n","y = all_epochs.events[:, -1]  # Assuming last column of events array contains the labels\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n","\n","# Class Splits\n","class_priors = [0.5, 0.5];\n","\n","# Create LDA model\n","lda = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto', priors=class_priors)\n","#try solver='eigen'\n","lda.fit(X_train, y_train)\n","\n","# -------------------------------------------------------------------------------------------------------\n","\n","# Model Validation and Evaluation\n","\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n","\n","# Predict on test data\n","y_pred_lda = lda.predict(X_test)\n","\n","# Evaluate the LDA model\n","accuracy_lda = accuracy_score(y_test, y_pred_lda)\n","conf_matrix_lda = confusion_matrix(y_test, y_pred_lda)\n","class_report_lda = classification_report(y_test, y_pred_lda)\n","\n","print(f\"LDA Accuracy: {accuracy_lda}\\n\")\n","print(f\"LDA Confusion Matrix:\\n{conf_matrix_lda}\\n\")\n","print(f\"LDA Classification Report:\\n{class_report_lda}\\n\")"],"metadata":{"id":"TV8GXiHZy5tc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LDA Visualization"],"metadata":{"id":"UCzETj9mvyi-"}},{"cell_type":"code","source":["import seaborn as sns\n","\n","# Assuming lda is your trained model and X_test, y_test are your test data\n","X_test_transformed = lda.transform(X_test)\n","sns.scatterplot(x=X_test_transformed[:, 0], y=y_test)  # Adjust indices for different components\n","plt.xlabel('LDA Component 1')\n","plt.title('LDA Class Separation')\n","plt.show()"],"metadata":{"id":"RxlSRr6Yv2pW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Random Forest Model Building\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Preparing labels for classification\n","# Assuming last column of events array contains the labels\n","y = all_epochs.events[:, -1]\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_pca, y,\n","                                                    test_size=0.3,\n","                                                    random_state=42)\n","\n","# Create RF Model\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X_train, y_train)\n","\n","# -------------------------------------------------------------------------------------------------------\n","\n","# Model Validation and Evaluation\n","\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n","\n","# Predict on test data\n","y_pred_rf = rf.predict(X_test)\n","\n","# Evaluate the Random Forest model\n","accuracy_rf = accuracy_score(y_test, y_pred_rf)\n","conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n","class_report_rf = classification_report(y_test, y_pred_rf)\n","\n","print(f\"Random Forest Accuracy: {accuracy_rf}\\n\")\n","print(f\"Random Forest Confusion Matrix:\\n{conf_matrix_rf}\\n\")\n","print(f\"Random Forest Classification Report:\\n{class_report_rf}\\n\")"],"metadata":{"id":"k1vXHmSVWG8q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Finding Best RF Parameters"],"metadata":{"id":"pp7XI3gn8C5g"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","# Parameters to tune\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [None, 10, 20, 30],\n","    'min_samples_split': [2, 5, 10]\n","}\n","\n","# Grid Search with cross-validation\n","rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n","rf_grid.fit(X_train, y_train)\n","\n","# Use the best estimator for predictions\n","y_pred_rf = rf_grid.best_estimator_.predict(X_test)"],"metadata":{"id":"n2ZBYVT03DMi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","# Print the best parameters and best score\n","print(\"Best Parameters:\", rf_grid.best_params_)\n","print(\"Best Training Score:\", rf_grid.best_score_)\n","\n","# Evaluate on the test set\n","y_pred_rf = rf_grid.best_estimator_.predict(X_test)\n","accuracy_rf = accuracy_score(y_test, y_pred_rf)\n","conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n","class_report_rf = classification_report(y_test, y_pred_rf)\n","\n","# Print test set evaluation metrics\n","print(f\"Random Forest Test Set Accuracy: {accuracy_rf}\")\n","print(f\"Random Forest Confusion Matrix on Test Set:\\n{conf_matrix_rf}\")\n","print(f\"Random Forest Classification Report on Test Set:\\n{class_report_rf}\")\n"],"metadata":{"id":"BxooT5iY3vRT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CNN Prototype"],"metadata":{"id":"PRh6PkIb27OB"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","\n","# Reshaping the data for CNN input\n","\n","# Accessing the data from the Epochs object\n","data = all_epochs.get_data()\n","# Data is a 3D array of shape (n_epochs, n_channels, n_times)\n","n_epochs, n_channels, n_times = data.shape\n","\n","X_cnn = data.reshape((n_epochs, n_channels, n_times, 1))\n","\n","# Convert y to a continuous range starting from 0\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","\n","# Convert the encoded labels to categorical (one-hot encoding)\n","y_categorical = to_categorical(y_encoded)\n","\n","# Splitting the data\n","X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y_categorical, test_size=0.3, random_state=42)\n","\n","# CNN Model\n","model = Sequential()\n","model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(n_channels, n_times, 1)))\n","model.add(MaxPooling2D(pool_size=2))\n","model.add(Conv2D(64, kernel_size=3, activation='relu'))\n","model.add(MaxPooling2D(pool_size=2))\n","model.add(Flatten())\n","model.add(Dense(100, activation='relu'))\n","model.add(Dense(len(np.unique(y)), activation='softmax')) # Number of classes\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train_cnn, y_train_cnn, validation_data=(X_test_cnn, y_test_cnn), epochs=10)\n","\n","# Evaluate the model\n","accuracy_cnn = model.evaluate(X_test_cnn, y_test_cnn)[1]\n","print(f\"CNN Model Accuracy: {accuracy_cnn}\")"],"metadata":{"id":"pfiVoqG3m17o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","# Make predictions\n","y_pred_cnn = model.predict(X_test_cnn)\n","# Convert predictions from one-hot encoded back to labels\n","y_pred_labels = np.argmax(y_pred_cnn, axis=1)\n","\n","# Convert test labels from one-hot encoded back to labels\n","y_test_labels = np.argmax(y_test_cnn, axis=1)\n","\n","# Compute confusion matrix\n","conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n","print(conf_matrix)\n","\n","# Print classification report\n","print(classification_report(y_test_labels, y_pred_labels))"],"metadata":{"id":"sbU-PKWyuRh1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Sending Model to Raspberry Pi as JOBLIB File"],"metadata":{"id":"w2HaJZMk8rUA"}},{"cell_type":"code","source":["!pip install joblib"],"metadata":{"id":"BrQj6p8o8uy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.datasets import load_iris\n","import joblib\n","\n","# Save the model to a file\n","joblib.dump(lda, 'ldamodelcategorical.joblib')"],"metadata":{"id":"wcBwZQM18zPN"},"execution_count":null,"outputs":[]}]}